base_model:
  name: "TinyLlama-1.1B-Chat-v1.0"
  revision: "main"
  trust_remote_code: false
  device: "cuda"  # or "cpu"
  dtype: "float16"  # or "float32", "bfloat16"

lora:
  enabled: true
  r: 8
  alpha: 32
  dropout: 0.1
  target_modules: ["query_key_value"]
  bias: "none"
  task_type: "CAUSAL_LM"

quantization:
  enabled: true
  bits: 4
  double_quant: true
  quant_type: "nf4"