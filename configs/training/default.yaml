optimizer:
  name: "adamw"
  lr: 2e-4
  weight_decay: 0.01
  
scheduler:
  name: "cosine"
  num_warmup_steps: 100
  
training:
  num_epochs: 3
  gradient_accumulation_steps: 4
  gradient_clipping: 1.0
  eval_steps: 50
  logging_steps: 10
  save_steps: 50
  fp16: true

evaluation:
  metric: "accuracy"
  batch_size: 8