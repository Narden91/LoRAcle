model:
  model:
    name: TinyLlama-1.1B-Chat-v1.0
    use_qlora: true
    lora_config:
      r: 8
      lora_alpha: 32
      lora_dropout: 0.1
      bias: none
      task_type: CAUSAL_LM
      target_modules:
      - query_key_value
data:
  data:
    dataset_name: databricks/dolly_subset
    storage_type: local
    local_path: ${hydra:runtime.cwd}/data
    s3_config:
      bucket_name: your-bucket
      aws_region: us-east-1
      access_key: ${oc.env:AWS_ACCESS_KEY_ID}
      secret_key: ${oc.env:AWS_SECRET_ACCESS_KEY}
    train_split: train
    eval_split: validation
    max_length: 512
    batch_size: 4
training:
  training:
    num_epochs: 3
    learning_rate: 0.0002
    gradient_accumulation_steps: 4
    warmup_steps: 10
    logging_steps: 10
    save_steps: 50
    fp16: true
    evaluation_strategy: steps
    eval_steps: 50
mode: train
print_config: true
experiment:
  name: ${now:%Y-%m-%d_%H-%M-%S}
  dir: ${hydra:runtime.cwd}/outputs/${experiment.name}
  save_dir: ${experiment.dir}/checkpoints
