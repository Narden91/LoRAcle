model:
  model:
    name: TinyLlama-1.1B-Chat-v1.0
    use_qlora: true
    lora_config:
      r: 8
      lora_alpha: 32
      lora_dropout: 0.1
      bias: none
      task_type: CAUSAL_LM
      target_modules:
      - query_key_value
data:
  data:
    dataset:
      name: databricks/dolly_subset
      train_split: train
      eval_split: validation
      max_length: 512
    storage:
      type: local
      local:
        data_dir: ${hydra:runtime.cwd}/data
        cache_dir: ${hydra:runtime.cwd}/data/cache
      s3:
        bucket_name: your-bucket
        aws_region: us-east-1
        access_key: ${oc.env:AWS_ACCESS_KEY_ID}
        secret_key: ${oc.env:AWS_SECRET_ACCESS_KEY}
    preprocessing:
      batch_size: 4
      num_workers: 2
      shuffle: true
training:
  training:
    num_epochs: 3
    learning_rate: 0.0002
    gradient_accumulation_steps: 4
    warmup_steps: 10
    logging_steps: 10
    save_steps: 50
    fp16: true
    evaluation_strategy: steps
    eval_steps: 50
mode: train
print_config: true
experiment:
  name: ${now:%Y-%m-%d_%H-%M-%S}
  dir: ${hydra:runtime.cwd}/outputs/${experiment.name}
  save_dir: ${experiment.dir}/checkpoints
